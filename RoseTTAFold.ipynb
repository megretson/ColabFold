{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoseTTAFold.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8ecf5d21a2141989539408e9d177afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71065a3edf204fec9a39edb6be85ce73",
              "IPY_MODEL_534c607b503a458595144edd3cb9aad1",
              "IPY_MODEL_b555aefc7cc74dcd8e33660e225fbee9"
            ],
            "layout": "IPY_MODEL_10f9e94eb0374033a5d313c429e2c9b4"
          }
        },
        "71065a3edf204fec9a39edb6be85ce73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf481a7ef9aa4205a26bf0436d6ced23",
            "placeholder": "​",
            "style": "IPY_MODEL_345b6e46a59145d58b81ad76f01e13b4",
            "value": "COMPLETE: 100%"
          }
        },
        "534c607b503a458595144edd3cb9aad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8bd9ac8d0554816b338611ae153c125",
            "max": 150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a6dc8f8f1f54664a6c54d1e89733b05",
            "value": 150
          }
        },
        "b555aefc7cc74dcd8e33660e225fbee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbb36bb672ac4810bc53de0c35dcf3a7",
            "placeholder": "​",
            "style": "IPY_MODEL_e31b485f39ca4ce78b0b4a12c67adf25",
            "value": " 150/150 [elapsed: 00:02 remaining: 00:00]"
          }
        },
        "10f9e94eb0374033a5d313c429e2c9b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf481a7ef9aa4205a26bf0436d6ced23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "345b6e46a59145d58b81ad76f01e13b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8bd9ac8d0554816b338611ae153c125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a6dc8f8f1f54664a6c54d1e89733b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bbb36bb672ac4810bc53de0c35dcf3a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e31b485f39ca4ce78b0b4a12c67adf25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/megretson/ColabFold/blob/main/RoseTTAFold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74RP4ByMFXHR"
      },
      "source": [
        "# RoseTTAFold\n",
        "**Limitations**\n",
        "- This notebook disables a few aspects (templates, pytosetta) of the full rosettafold pipeline.\n",
        "- For best resuls use the [full pipeline](https://github.com/RosettaCommons/RoseTTAFold) or [Robetta webserver](https://robetta.bakerlab.org/)!\n",
        "- For a typical Google-Colab session, with a `16G-GPU`, the max total length is **700 residues**. Sometimes a `12G-GPU` is assigned, in which case the max length is lower.\n",
        "- For version of RoseTTAFold that runs with pyRosetta [see here](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/RoseTTAFold.ipynb).\n",
        "\n",
        "For other related notebooks see [ColabFold](https://github.com/sokrypton/ColabFold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1G82GuO-tez",
        "outputId": "14ac7eeb-5e61-493e-8e9f-6a1dae0953c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v06TAo9ZaGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c7bc05-802c-4a1b-ab31-49bf7c0a19a4"
      },
      "source": [
        "#@title ##Install and import libraries\n",
        "#@markdown This step can take up to ~2 mins\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from IPython.utils import io\n",
        "from google.colab import files\n",
        "!pip install biopython\n",
        "from google.colab import drive\n",
        "from Bio import SeqIO\n",
        "\n",
        "import torch\n",
        "torch_v = torch.__version__\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "if not os.path.isdir(\"RoseTTAFold\"):\n",
        "  with io.capture_output() as captured:\n",
        "    # extra functionality\n",
        "    %shell wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/colabfold.py\n",
        "\n",
        "    # download model\n",
        "    %shell git clone https://github.com/RosettaCommons/RoseTTAFold.git\n",
        "    %shell wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/RoseTTAFold__network__Refine_module.patch\n",
        "    %shell patch -u RoseTTAFold/network/Refine_module.py -i RoseTTAFold__network__Refine_module.patch\n",
        "\n",
        "    # download model params\n",
        "    %shell wget -qnc https://files.ipd.uw.edu/pub/RoseTTAFold/weights.tar.gz\n",
        "    %shell tar -xf weights.tar.gz\n",
        "    %shell rm weights.tar.gz\n",
        "\n",
        "    # download scwrl4 (for adding sidechains)\n",
        "    # http://dunbrack.fccc.edu/SCWRL3.php\n",
        "    # Thanks Roland Dunbrack!\n",
        "    %shell wget -qnc https://files.ipd.uw.edu/krypton/TrRosetta/scwrl4.zip\n",
        "    %shell unzip -qqo scwrl4.zip\n",
        "\n",
        "    # install libraries\n",
        "    %shell pip install -q dgl-cu113 -f https://data.dgl.ai/wheels/repo.html\n",
        "    %shell pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-{torch_v}.html\n",
        "    %shell pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-{torch_v}.html\n",
        "    %shell pip install -q torch-geometric\n",
        "    %shell pip install -q py3Dmol\n",
        "    %shell pip install biopython\n",
        "\n",
        "with io.capture_output() as captured:\n",
        "  sys.path.append('/content/RoseTTAFold/network')\n",
        "  import predict_e2e\n",
        "  from parsers import parse_a3m\n",
        "  \n",
        "import colabfold as cf\n",
        "import py3Dmol\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_bfactor(pdb_filename):\n",
        "  bfac = []\n",
        "  for line in open(pdb_filename,\"r\"):\n",
        "    if line[:4] == \"ATOM\":\n",
        "      bfac.append(float(line[60:66]))\n",
        "  return np.array(bfac)\n",
        "\n",
        "def set_bfactor(pdb_filename, bfac):\n",
        "  I = open(pdb_filename,\"r\").readlines()\n",
        "  O = open(pdb_filename,\"w\")\n",
        "  for line in I:\n",
        "    if line[0:6] == \"ATOM  \":\n",
        "      seq_id = int(line[22:26].strip()) - 1\n",
        "      O.write(f\"{line[:60]}{bfac[seq_id]:6.2f}{line[66:]}\")\n",
        "  O.close()    \n",
        "\n",
        "def do_scwrl(inputs, outputs, exe=\"./scwrl4/Scwrl4\"):\n",
        "  subprocess.run([exe,\"-i\",inputs,\"-o\",outputs,\"-h\"],\n",
        "                  stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "  bfact = get_bfactor(inputs)\n",
        "  set_bfactor(outputs, bfact)\n",
        "  return bfact"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.81-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from biopython) (1.22.4)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.81\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nTsyKtQxjO8"
      },
      "source": [
        "#@title Search against genetic databases\n",
        "#@markdown ---\n",
        "msa_method = \"mmseqs2\" #@param [\"mmseqs2\",\"single_sequence\",\"custom_a3m\"]\n",
        "#@markdown - `mmseqs2` - FAST method from [ColabFold](https://github.com/sokrypton/ColabFold)\n",
        "#@markdown - `single_sequence` - use single sequence input (not recommended, unless a *denovo* design and you dont expect to find any homologous sequences)\n",
        "#@markdown - `custom_a3m` Upload custom MSA (a3m format)\n",
        "\n",
        "\n",
        "def search_against_databases(jobname, sequence, msa_method, memory_gobbler):\n",
        "  # tmp directory\n",
        "  prefix = cf.get_hash(sequence)\n",
        "  os.makedirs('tmp', exist_ok=True)\n",
        "  prefix = os.path.join('tmp',prefix)\n",
        "\n",
        "  os.makedirs(jobname, exist_ok=True)\n",
        "\n",
        "\n",
        "  if msa_method == \"mmseqs2\":\n",
        "    a3m_lines = cf.run_mmseqs2(sequence, prefix, filter=True)\n",
        "    with open(f\"{jobname}/msa.a3m\",\"w\") as a3m:\n",
        "      a3m.write(a3m_lines)\n",
        "\n",
        "  elif msa_method == \"single_sequence\":\n",
        "    with open(f\"{jobname}/msa.a3m\",\"w\") as a3m:\n",
        "      a3m.write(f\">{jobname}\\n{sequence}\\n\")\n",
        "\n",
        "  elif msa_method == \"custom_a3m\":\n",
        "    print(\"upload custom a3m\")\n",
        "    msa_dict = files.upload()\n",
        "    lines = msa_dict[list(msa_dict.keys())[0]].decode().splitlines()\n",
        "    a3m_lines = []\n",
        "    for line in lines:\n",
        "      line = line.replace(\"\\x00\",\"\")\n",
        "      if len(line) > 0 and not line.startswith('#'):\n",
        "        a3m_lines.append(line)\n",
        "\n",
        "    with open(f\"{jobname}/msa.a3m\",\"w\") as a3m:\n",
        "      a3m.write(\"\\n\".join(a3m_lines))\n",
        "\n",
        "  msa_all = parse_a3m(f\"{jobname}/msa.a3m\")\n",
        "  msa_arr = np.unique(msa_all,axis=0)\n",
        "  total_msa_size = len(msa_arr)\n",
        "  if msa_method == \"mmseqs2\":\n",
        "    print(f'\\n{total_msa_size} Sequences Found in Total (after filtering)\\n')\n",
        "  else:\n",
        "    print(f'\\n{total_msa_size} Sequences Found in Total\\n')\n",
        "\n",
        "  if total_msa_size > 1 and memory_gobbler:\n",
        "    plt.figure(figsize=(8,5),dpi=100)\n",
        "    plt.title(\"Sequence coverage\")\n",
        "    seqid = (msa_all[0] == msa_arr).mean(-1)\n",
        "    seqid_sort = seqid.argsort()\n",
        "    non_gaps = (msa_arr != 20).astype(float)\n",
        "    non_gaps[non_gaps == 0] = np.nan\n",
        "\n",
        "    plt.imshow(non_gaps[seqid_sort]*seqid[seqid_sort,None],\n",
        "              interpolation='nearest', aspect='auto',\n",
        "              cmap=\"rainbow_r\", vmin=0, vmax=1, origin='lower',\n",
        "              extent=(0, msa_arr.shape[1], 0, msa_arr.shape[0]))\n",
        "    plt.plot((msa_arr != 20).sum(0), color='black')\n",
        "    plt.xlim(0,msa_arr.shape[1])\n",
        "    plt.ylim(0,msa_arr.shape[0])\n",
        "    plt.colorbar(label=\"Sequence identity to query\",)\n",
        "    plt.xlabel(\"Positions\")\n",
        "    plt.ylabel(\"Sequences\")\n",
        "    plt.savefig(f\"{jobname}/msa_coverage.png\", bbox_inches = 'tight')\n",
        "    plt.show()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14-q4hv59ast"
      },
      "source": [
        "#@title ## Run RoseTTAFold for mainchain and Scrwl4 for sidechain prediction\n",
        "\n",
        "def run_rosettafold(jobname, memory_gobbler):\n",
        "  # load model\n",
        "  if \"rosettafold\" not in dir():\n",
        "    rosettafold = predict_e2e.Predictor(model_dir=\"weights\")\n",
        "\n",
        "  # make prediction using model\n",
        "  rosettafold.predict(f\"{jobname}/msa.a3m\",f\"{jobname}/pred\")\n",
        "\n",
        "  # pack sidechains using Scwrl4\n",
        "  plddt = do_scwrl(f\"{jobname}/pred.pdb\",f\"{jobname}/pred.scwrl.pdb\")\n",
        "\n",
        "  print(f\"Predicted LDDT: {plddt.mean()}\")\n",
        "  if memory_gobbler:\n",
        "    plt.figure(figsize=(8,5),dpi=100)\n",
        "    plt.plot(plddt)\n",
        "    plt.xlabel(\"positions\")\n",
        "    plt.ylabel(\"plddt\")\n",
        "    plt.ylim(0,1)\n",
        "    plt.savefig(f\"{jobname}/plddt.png\", bbox_inches = 'tight')\n",
        "    plt.show()\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPhmF8SZr1k8"
      },
      "source": [
        "def display_structure(jobname):\n",
        "  #@title Display 3D structure {run: \"auto\"}\n",
        "  color = \"lDDT\" #@param [\"chain\", \"lDDT\", \"rainbow\"]\n",
        "  show_sidechains = False #@param {type:\"boolean\"}\n",
        "  show_mainchains = False #@param {type:\"boolean\"}\n",
        "  cf.show_pdb(f\"{jobname}/pred.scwrl.pdb\", show_sidechains, show_mainchains, color, chains=1, vmin=0.5, vmax=0.9).show()\n",
        "\n",
        "  if color == \"lDDT\": cf.plot_plddt_legend().show()  "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0RhMLHKHf8d"
      },
      "source": [
        "#@title Download prediction\n",
        "\n",
        "#@markdown Once this cell has been executed, a zip-archive with \n",
        "#@markdown the obtained prediction will be automatically downloaded \n",
        "#@markdown to your computer.\n",
        "\n",
        "def save_to_drive(jobname, sequence, msa_method):\n",
        "  # add settings file\n",
        "  %cd /content/\n",
        "  !zip -r /content/{jobname}.zip /content/{jobname}/\n",
        "  %cd /content/gdrive/My\\ Drive/\n",
        "  !mkdir -p ./collab/{jobname}/\n",
        "  ! mv ../../{jobname}.zip ./collab/{jobname}/\n",
        "  settings_path = \"./collab/{}/settings.txt\".format(jobname)\n",
        "\n",
        "  with open(settings_path, \"w\") as text_file:\n",
        "    text_file.write(f\"method=RoseTTAFold\\n\")\n",
        "    text_file.write(f\"sequence={sequence}\\n\")\n",
        "    text_file.write(f\"msa_method={msa_method}\\n\")\n",
        "    text_file.write(f\"use_templates=False\\n\")\n",
        "  \n",
        "  !rm -r /content/{jobname}/\n",
        "  print(\"Saved job {} to drive\".format(jobname))\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reference for later, in case it's useful: https://stackoverflow.com/questions/47320052/load-local-data-files-to-colaboratory\n",
        "%cd /content/\n",
        "msa_method = \"mmseqs2\"\n",
        "sequences = SeqIO.parse(\"all_seqs.fasta\", \"fasta\")\n",
        "max = 500\n",
        "count = 0\n",
        "memory_gobbler = False\n",
        "\n",
        "for seq_record in sequences:\n",
        "  if count < max:\n",
        "    sequence = str(seq_record.seq)\n",
        "    sequence = sequence.translate(str.maketrans('', '', ' \\n\\t')).upper()\n",
        "    sequence_id =  seq_record.id.split(\"_\")[0]\n",
        "    jobname = \"test_\" + sequence_id\n",
        "    print(\"Starting: {}\".format(jobname))\n",
        "    %cd /content/gdrive/My\\ Drive/\n",
        "    if not (os.path.exists(\"./collab/{}\".format(jobname))):\n",
        "      print(\"New prediction\")\n",
        "      %cd /content/\n",
        "      search_against_databases(jobname, sequence, msa_method, memory_gobbler)\n",
        "      run_rosettafold(jobname, memory_gobbler)\n",
        "      if memory_gobbler:\n",
        "        display_structure(jobname)\n",
        "      save_to_drive(jobname, sequence, msa_method)\n",
        "    else:\n",
        "      print(\"This prediction ({}) already exists on file: skipping\".format(jobname))\n",
        "    count += 1 \n",
        "  else:\n",
        "    break\n",
        "    \n",
        "    \n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503,
          "referenced_widgets": [
            "f8ecf5d21a2141989539408e9d177afb",
            "71065a3edf204fec9a39edb6be85ce73",
            "534c607b503a458595144edd3cb9aad1",
            "b555aefc7cc74dcd8e33660e225fbee9",
            "10f9e94eb0374033a5d313c429e2c9b4",
            "bf481a7ef9aa4205a26bf0436d6ced23",
            "345b6e46a59145d58b81ad76f01e13b4",
            "c8bd9ac8d0554816b338611ae153c125",
            "8a6dc8f8f1f54664a6c54d1e89733b05",
            "bbb36bb672ac4810bc53de0c35dcf3a7",
            "e31b485f39ca4ce78b0b4a12c67adf25"
          ]
        },
        "id": "xidcVmTZPEKG",
        "outputId": "79c143c3-c0c5-4477-da26-8a287703c1b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Starting: test_4FPQ\n",
            "/content/gdrive/My Drive\n",
            "This prediction (test_4FPQ) already exists on file: skipping\n",
            "Starting: test_7FC7\n",
            "/content/gdrive/My Drive\n",
            "This prediction (test_7FC7) already exists on file: skipping\n",
            "Starting: test_7FJM\n",
            "/content/gdrive/My Drive\n",
            "New prediction\n",
            "/content\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/150 [elapsed: 00:00 remaining: ?]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8ecf5d21a2141989539408e9d177afb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "8497 Sequences Found in Total (after filtering)\n",
            "\n",
            "ngrid:      15\n",
            "grids:      [   0   75  150  225  300  375  450  525  600  675  750  825  900  975\n",
            " 1050]\n",
            "windows:    150\n",
            "running crop: 0-150/0-150 torch.Size([1, 1000, 150])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/dgl/backend/pytorch/tensor.py:352: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  assert input.numel() == input.storage().size(), \"Cannot convert view \" \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running crop: 0-150/75-225 torch.Size([1, 1000, 225])\n",
            "running crop: 0-150/150-300 torch.Size([1, 1000, 300])\n",
            "running crop: 0-150/225-375 torch.Size([1, 1000, 300])\n",
            "running crop: 0-150/300-450 torch.Size([1, 1000, 300])\n"
          ]
        }
      ]
    }
  ]
}